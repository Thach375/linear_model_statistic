{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3266a25c",
   "metadata": {},
   "source": [
    "Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e5058d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold, train_test_split, RandomizedSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.preprocessing import PolynomialFeatures, OneHotEncoder, StandardScaler, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from scipy.stats import uniform\n",
    "import joblib\n",
    "import json\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9375c73",
   "metadata": {},
   "source": [
    "Load data and filling missing value with median of the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "274f4f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading important features from feature_importance.csv...\n",
      "Warning: feature_importance.csv not found. Using all features.\n"
     ]
    }
   ],
   "source": [
    "# Load feature importance data\n",
    "print(\"Loading important features from feature_importance.csv...\")\n",
    "try:\n",
    "    feature_importance = pd.read_csv('models/feature_importance.csv')\n",
    "    # Get top features (you can adjust the number)\n",
    "    important_features = feature_importance['Feature'].tolist()\n",
    "    print(f\"Using features: {important_features}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Warning: feature_importance.csv not found. Using all features.\")\n",
    "    important_features = None\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('../data/train.csv')\n",
    "\n",
    "# Split features and target\n",
    "X_full = data.drop(columns=['SalePrice', 'Id'])\n",
    "y = data['SalePrice']\n",
    "\n",
    "# Filter for important features if available\n",
    "if important_features:\n",
    "    # Some features in the importance file might be one-hot encoded names\n",
    "    # We need to extract original column names before one-hot encoding\n",
    "    original_feature_names = []\n",
    "    for feature in important_features:\n",
    "        # Check if feature exists directly in the dataset\n",
    "        if feature in X_full.columns:\n",
    "            original_feature_names.append(feature)\n",
    "        else:\n",
    "            # This might be a one-hot encoded feature, extract the column name\n",
    "            for col in X_full.columns:\n",
    "                if col in feature:\n",
    "                    if col not in original_feature_names:\n",
    "                        original_feature_names.append(col)\n",
    "                        \n",
    "    X = X_full[original_feature_names]\n",
    "else:\n",
    "    X = X_full\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "numerical_cols = X.select_dtypes(exclude=['object']).columns\n",
    "\n",
    "# Define K-fold cross validation strategy\n",
    "k_folds = 5  # Number of folds\n",
    "cv = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Phân loại biến categorical thành có thứ tự và không có thứ tự\n",
    "ordinal_features = ['OverallQual', 'BsmtQual', 'KitchenQual']\n",
    "\n",
    "nominal_features = [col for col in categorical_cols if col not in ordinal_features]\n",
    "\n",
    "# Tạo transformers riêng cho từng loại dữ liệu\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "ordinal_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('ordinal', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "])\n",
    "\n",
    "nominal_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first'))  # drop='first' để giảm đa cộng tuyến\n",
    "])\n",
    "\n",
    "# Kết hợp các transformers\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_cols),\n",
    "        ('ord', ordinal_transformer, ordinal_features),\n",
    "        ('nom', nominal_transformer, nominal_features)\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12849056",
   "metadata": {},
   "source": [
    "Defined Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b316f820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models and hyperparameter distributions\n",
    "def new_func(preprocessor):\n",
    "    models = {\n",
    "    \"LinearRegression\": (\n",
    "        LinearRegression(),\n",
    "        {\n",
    "            \"model__fit_intercept\": [True, False]\n",
    "        },\n",
    "        preprocessor, \n",
    "        {}\n",
    "    ),\n",
    "    \"Ridge\": (\n",
    "        Ridge(random_state=42),\n",
    "        {\n",
    "            \"model__alpha\": uniform(0.01, 100),\n",
    "            \"model__fit_intercept\": [True, False]\n",
    "        },\n",
    "        preprocessor, \n",
    "        {}\n",
    "    ),\n",
    "    \"Lasso\": (\n",
    "        Lasso(random_state=42, max_iter=10000),\n",
    "        {\n",
    "            \"model__alpha\": uniform(0.01, 100),\n",
    "            \"model__fit_intercept\": [True, False]\n",
    "        },\n",
    "        preprocessor, \n",
    "        {}\n",
    "    ),\n",
    "    \"ElasticNet\": (\n",
    "        ElasticNet(random_state=42, max_iter=10000),\n",
    "        {\n",
    "            \"model__alpha\": uniform(0.01, 100),\n",
    "            \"model__l1_ratio\": uniform(0.0, 1.0),\n",
    "            \"model__fit_intercept\": [True, False]\n",
    "        },\n",
    "        preprocessor,\n",
    "        {}\n",
    "    ),\n",
    "    \"PolynomialRegression\": (\n",
    "        LinearRegression(),\n",
    "        {\n",
    "            \"prep__poly__degree\": [1, 2, 3],  # Fix the parameter path\n",
    "            \"model__fit_intercept\": [True, False]\n",
    "        },\n",
    "        Pipeline([\n",
    "            ('preprocess', preprocessor),\n",
    "            ('poly', PolynomialFeatures())\n",
    "        ]),\n",
    "        {}\n",
    "    )\n",
    "}\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b964806",
   "metadata": {},
   "source": [
    "Training Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1260c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "▶▶ LinearRegression: Randomized search (50 configs, 3-fold)\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Thach\\anaconda3\\envs\\Thong_ke\\lib\\site-packages\\sklearn\\model_selection\\_search.py:317: UserWarning: The total space of parameters 2 is smaller than n_iter=50. Running 2 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "↳ Done in 0.1 min — best RMSE=42940.2670\n",
      " mean_test_score                          params\n",
      "    42940.266968 {'model__fit_intercept': False}\n",
      "    42957.621135  {'model__fit_intercept': True}\n",
      "Cross-Validation RMSE: 42940.2670\n",
      "\n",
      "▶▶ Ridge: Randomized search (50 configs, 3-fold)\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "↳ Done in 0.1 min — best RMSE=31427.4791\n",
      " mean_test_score                                                              params\n",
      "    31427.479080 {'model__alpha': 13.959386065204184, 'model__fit_intercept': False}\n",
      "    31437.851480 {'model__alpha': 18.350450985343382, 'model__fit_intercept': False}\n",
      "    31437.866463  {'model__alpha': 18.35347898661638, 'model__fit_intercept': False}\n",
      "    31446.459756 {'model__alpha': 19.894240408880517, 'model__fit_intercept': False}\n",
      "    31460.991401  {'model__alpha': 14.102422497476264, 'model__fit_intercept': True}\n",
      "Cross-Validation RMSE: 31427.4791\n",
      "\n",
      "▶▶ Lasso: Randomized search (50 configs, 3-fold)\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "↳ Done in 1.2 min — best RMSE=31063.6413\n",
      " mean_test_score                                                             params\n",
      "    31063.641333  {'model__alpha': 97.38555188414593, 'model__fit_intercept': True}\n",
      "    31071.419151  {'model__alpha': 96.12720243493492, 'model__fit_intercept': True}\n",
      "    31108.274179 {'model__alpha': 98.69869366005173, 'model__fit_intercept': False}\n",
      "    31132.580905 {'model__alpha': 94.89855372533333, 'model__fit_intercept': False}\n",
      "    31250.704978 {'model__alpha': 83.25426408004218, 'model__fit_intercept': False}\n",
      "Cross-Validation RMSE: 31063.6413\n",
      "\n",
      "▶▶ ElasticNet: Randomized search (50 configs, 3-fold)\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "↳ Done in 0.1 min — best RMSE=31772.0541\n",
      " mean_test_score                                                                                                     params\n",
      "    31772.054071  {'model__alpha': 1.3364961159866529, 'model__fit_intercept': True, 'model__l1_ratio': 0.9656320330745594}\n",
      "    32616.802904   {'model__alpha': 16.53669390630025, 'model__fit_intercept': True, 'model__l1_ratio': 0.9868869366005173}\n",
      "    33091.838111  {'model__alpha': 1.4179822715084456, 'model__fit_intercept': True, 'model__l1_ratio': 0.7068573438476171}\n",
      "    33293.406748  {'model__alpha': 0.7166305219717406, 'model__fit_intercept': True, 'model__l1_ratio': 0.2912291401980419}\n",
      "    34948.863732 {'model__alpha': 2.0684494295802445, 'model__fit_intercept': False, 'model__l1_ratio': 0.7219987722668247}\n",
      "Cross-Validation RMSE: 31772.0541\n",
      "\n",
      "▶▶ PolynomialRegression: Randomized search (50 configs, 3-fold)\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Thach\\anaconda3\\envs\\Thong_ke\\lib\\site-packages\\sklearn\\model_selection\\_search.py:317: UserWarning: The total space of parameters 6 is smaller than n_iter=50. Running 6 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Thach\\anaconda3\\envs\\Thong_ke\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:516: FitFailedWarning: \n",
      "10 fits failed out of a total of 30.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "6 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Thach\\anaconda3\\envs\\Thong_ke\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Thach\\anaconda3\\envs\\Thong_ke\\lib\\site-packages\\sklearn\\base.py\", line 1363, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Thach\\anaconda3\\envs\\Thong_ke\\lib\\site-packages\\sklearn\\pipeline.py\", line 653, in fit\n",
      "    Xt = self._fit(X, y, routed_params, raw_params=params)\n",
      "  File \"c:\\Users\\Thach\\anaconda3\\envs\\Thong_ke\\lib\\site-packages\\sklearn\\pipeline.py\", line 587, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"c:\\Users\\Thach\\anaconda3\\envs\\Thong_ke\\lib\\site-packages\\joblib\\memory.py\", line 326, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"c:\\Users\\Thach\\anaconda3\\envs\\Thong_ke\\lib\\site-packages\\sklearn\\pipeline.py\", line 1539, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "  File \"c:\\Users\\Thach\\anaconda3\\envs\\Thong_ke\\lib\\site-packages\\sklearn\\base.py\", line 1363, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Thach\\anaconda3\\envs\\Thong_ke\\lib\\site-packages\\sklearn\\pipeline.py\", line 729, in fit_transform\n",
      "    return last_step.fit_transform(\n",
      "  File \"c:\\Users\\Thach\\anaconda3\\envs\\Thong_ke\\lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 316, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "  File \"c:\\Users\\Thach\\anaconda3\\envs\\Thong_ke\\lib\\site-packages\\sklearn\\base.py\", line 895, in fit_transform\n",
      "    return self.fit(X, y, **fit_params).transform(X)\n",
      "  File \"c:\\Users\\Thach\\anaconda3\\envs\\Thong_ke\\lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 316, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "  File \"c:\\Users\\Thach\\anaconda3\\envs\\Thong_ke\\lib\\site-packages\\sklearn\\preprocessing\\_polynomial.py\", line 499, in transform\n",
      "    XP = np.empty(\n",
      "numpy._core._exceptions._ArrayMemoryError: Unable to allocate 19.8 GiB for an array with shape (1168, 2275280) and data type float64\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Thach\\anaconda3\\envs\\Thong_ke\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Thach\\anaconda3\\envs\\Thong_ke\\lib\\site-packages\\sklearn\\base.py\", line 1363, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Thach\\anaconda3\\envs\\Thong_ke\\lib\\site-packages\\sklearn\\pipeline.py\", line 653, in fit\n",
      "    Xt = self._fit(X, y, routed_params, raw_params=params)\n",
      "  File \"c:\\Users\\Thach\\anaconda3\\envs\\Thong_ke\\lib\\site-packages\\sklearn\\pipeline.py\", line 587, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"c:\\Users\\Thach\\anaconda3\\envs\\Thong_ke\\lib\\site-packages\\joblib\\memory.py\", line 326, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"c:\\Users\\Thach\\anaconda3\\envs\\Thong_ke\\lib\\site-packages\\sklearn\\pipeline.py\", line 1539, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "  File \"c:\\Users\\Thach\\anaconda3\\envs\\Thong_ke\\lib\\site-packages\\sklearn\\base.py\", line 1363, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Thach\\anaconda3\\envs\\Thong_ke\\lib\\site-packages\\sklearn\\pipeline.py\", line 729, in fit_transform\n",
      "    return last_step.fit_transform(\n",
      "  File \"c:\\Users\\Thach\\anaconda3\\envs\\Thong_ke\\lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 316, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "  File \"c:\\Users\\Thach\\anaconda3\\envs\\Thong_ke\\lib\\site-packages\\sklearn\\base.py\", line 895, in fit_transform\n",
      "    return self.fit(X, y, **fit_params).transform(X)\n",
      "  File \"c:\\Users\\Thach\\anaconda3\\envs\\Thong_ke\\lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 316, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "  File \"c:\\Users\\Thach\\anaconda3\\envs\\Thong_ke\\lib\\site-packages\\sklearn\\preprocessing\\_polynomial.py\", line 499, in transform\n",
      "    XP = np.empty(\n",
      "numpy._core._exceptions._ArrayMemoryError: Unable to allocate 20.0 GiB for an array with shape (1168, 2303960) and data type float64\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Thach\\anaconda3\\envs\\Thong_ke\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Thach\\anaconda3\\envs\\Thong_ke\\lib\\site-packages\\sklearn\\base.py\", line 1363, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Thach\\anaconda3\\envs\\Thong_ke\\lib\\site-packages\\sklearn\\pipeline.py\", line 653, in fit\n",
      "    Xt = self._fit(X, y, routed_params, raw_params=params)\n",
      "  File \"c:\\Users\\Thach\\anaconda3\\envs\\Thong_ke\\lib\\site-packages\\sklearn\\pipeline.py\", line 587, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"c:\\Users\\Thach\\anaconda3\\envs\\Thong_ke\\lib\\site-packages\\joblib\\memory.py\", line 326, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"c:\\Users\\Thach\\anaconda3\\envs\\Thong_ke\\lib\\site-packages\\sklearn\\pipeline.py\", line 1539, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "  File \"c:\\Users\\Thach\\anaconda3\\envs\\Thong_ke\\lib\\site-packages\\sklearn\\base.py\", line 1363, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Thach\\anaconda3\\envs\\Thong_ke\\lib\\site-packages\\sklearn\\pipeline.py\", line 729, in fit_transform\n",
      "    return last_step.fit_transform(\n",
      "  File \"c:\\Users\\Thach\\anaconda3\\envs\\Thong_ke\\lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 316, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "  File \"c:\\Users\\Thach\\anaconda3\\envs\\Thong_ke\\lib\\site-packages\\sklearn\\base.py\", line 895, in fit_transform\n",
      "    return self.fit(X, y, **fit_params).transform(X)\n",
      "  File \"c:\\Users\\Thach\\anaconda3\\envs\\Thong_ke\\lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 316, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "  File \"c:\\Users\\Thach\\anaconda3\\envs\\Thong_ke\\lib\\site-packages\\sklearn\\preprocessing\\_polynomial.py\", line 499, in transform\n",
      "    XP = np.empty(\n",
      "numpy._core._exceptions._ArrayMemoryError: Unable to allocate 18.1 GiB for an array with shape (1168, 2081156) and data type float64\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Thach\\anaconda3\\envs\\Thong_ke\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1135: UserWarning: One or more of the test scores are non-finite: [-42957.62113545 -35651.97977444             nan -42942.07958732\n",
      " -36093.78885461             nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "↳ Done in 0.5 min — best RMSE=35651.9798\n",
      " mean_test_score                                                   params\n",
      "    35651.979774  {'prep__poly__degree': 2, 'model__fit_intercept': True}\n",
      "    36093.788855 {'prep__poly__degree': 2, 'model__fit_intercept': False}\n",
      "    42942.079587 {'prep__poly__degree': 1, 'model__fit_intercept': False}\n",
      "    42957.621135  {'prep__poly__degree': 1, 'model__fit_intercept': True}\n",
      "             NaN  {'prep__poly__degree': 3, 'model__fit_intercept': True}\n",
      "Cross-Validation RMSE: 35651.9798\n",
      "\n",
      "Best model = Lasso  (val RMSE=31063.6413)\n"
     ]
    }
   ],
   "source": [
    "models = new_func(preprocessor)\n",
    "\n",
    "# Train and validate models\n",
    "best_pipe, best_score, best_name = None, float('inf'), \"\"\n",
    "\n",
    "os.makedirs(\"../models\", exist_ok=True)\n",
    "\n",
    "for name, (est, dist, prep, fit_kw) in models.items():\n",
    "    n_iter = 50\n",
    "    print(f\"\\n▶▶ {name}: Randomized search ({n_iter} configs, 3-fold)\")\n",
    "\n",
    "    # Create the pipeline with prep only if it's provided\n",
    "    if prep is None:\n",
    "        pipe = Pipeline([('model', est)])\n",
    "    else:\n",
    "        pipe = Pipeline([('prep', prep), ('model', est)])\n",
    "        \n",
    "    search = RandomizedSearchCV(\n",
    "        pipe, dist, n_iter=n_iter,\n",
    "        scoring='neg_root_mean_squared_error', n_jobs=4, random_state=42, verbose=1, refit=True\n",
    "    )\n",
    "    tic = time.time()\n",
    "    try:\n",
    "        search.fit(X, y, **fit_kw)\n",
    "    except Exception as e:\n",
    "        print(f\"↳ Error in {name}: {str(e)}\")\n",
    "        continue\n",
    "    toc = time.time()\n",
    "    print(f\"↳ Done in {(toc-tic)/60:.1f} min — best RMSE={-search.best_score_:.4f}\")\n",
    "\n",
    "    # Save best hyperparameters\n",
    "    with open(f\"../models/{name}_best.json\", \"w\") as fp:\n",
    "        json.dump(search.best_params_, fp, indent=2, default=str)\n",
    "\n",
    "    # Print top-5 configurations\n",
    "    top5 = (pd.DataFrame(search.cv_results_)\n",
    "            .sort_values(\"rank_test_score\")\n",
    "            .head(5)[[\"mean_test_score\", \"params\"]])\n",
    "    top5['mean_test_score'] = -top5['mean_test_score']  # Convert to positive RMSE\n",
    "    print(top5.to_string(index=False))\n",
    "\n",
    "    # K-fold validation is already handled in RandomizedSearchCV\n",
    "    # We'll use the best CV score as our validation metric\n",
    "    cv_rmse = -search.best_score_  # Convert back to positive RMSE\n",
    "    print(f\"Cross-Validation RMSE: {cv_rmse:.4f}\")    # Update best model\n",
    "    if cv_rmse < best_score:\n",
    "        best_score, best_pipe, best_name = cv_rmse, search.best_estimator_, name\n",
    "\n",
    "    # Save checkpoint\n",
    "    joblib.dump(search.best_estimator_, f\"../models/{name}_best.pkl\")\n",
    "\n",
    "# Final report\n",
    "if best_pipe is None:\n",
    "    raise RuntimeError(\"No model trained successfully!\")\n",
    "\n",
    "print(f\"\\nBest model = {best_name}  (val RMSE={best_score:.4f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Thong_ke",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
